{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Language Translation\n",
    "In this project, you’re going to take a peek into the realm of neural network machine translation.  You’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.\n",
    "## Get the Data\n",
    "The dataset is already in this repository as files, so there is no extra steps to download. What I need to do is to load the files. There are 2 files, and each contains bunch of sentences in English and French respectively. They are just plain text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "The two datasets store bunch of sentences in different language, and that is something we don't have to explore for now. You probably already know how your data looks like when you decided to download this one. **However**, it is worthwhile to explore how complex the datasets are. The complexity could suggest how we should approach to get the right result still considering some of restrictions. \n",
    "\n",
    "`note: ` The two files exactly contains the same number of lines. Each i-th line in both files has the same meaning but expressed in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Brief Stats\n",
      "* number of unique words in English sample sentences: 227        [this is roughly measured/without any preprocessing]\n",
      "\n",
      "* English sentences\n",
      "\t- number of sentences: 137861\n",
      "\t- avg. number of words in a sentence: 13.225277634719028\n",
      "* French sentences\n",
      "\t- number of sentences: 137861 [data integrity check / should have the same number]\n",
      "\t- avg. number of words in a sentence: 14.226612312401622\n",
      "\n",
      "* Sample sentences range from 0 to 5\n",
      "[1-th] sentence\n",
      "\tEN: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\tFR: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "[2-th] sentence\n",
      "\tEN: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\tFR: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "[3-th] sentence\n",
      "\tEN: california is usually quiet during march , and it is usually hot in june .\n",
      "\tFR: california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n",
      "[4-th] sentence\n",
      "\tEN: the united states is sometimes mild during june , and it is cold in september .\n",
      "\tFR: les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\n",
      "[5-th] sentence\n",
      "\tEN: your least liked fruit is the grape , but my least liked is the apple .\n",
      "\tFR: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Dataset Brief Stats')\n",
    "print('* number of unique words in English sample sentences: {}\\\n",
    "        [this is roughly measured/without any preprocessing]'.format(len(Counter(source_text.split()))))\n",
    "print()\n",
    "\n",
    "english_sentences = source_text.split('\\n')\n",
    "print('* English sentences')\n",
    "print('\\t- number of sentences: {}'.format(len(english_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in english_sentences])))\n",
    "\n",
    "french_sentences = target_text.split('\\n')\n",
    "print('* French sentences')\n",
    "print('\\t- number of sentences: {} [data integrity check / should have the same number]'.format(len(french_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in french_sentences])))\n",
    "print()\n",
    "\n",
    "sample_sentence_range = (0, 5)\n",
    "side_by_side_sentences = list(zip(english_sentences, french_sentences))[sample_sentence_range[0]:sample_sentence_range[1]]\n",
    "print('* Sample sentences range from {} to {}'.format(sample_sentence_range[0], sample_sentence_range[1]))\n",
    "\n",
    "for index, sentence in enumerate(side_by_side_sentences):\n",
    "    en_sent, fr_sent = sentence\n",
    "    print('[{}-th] sentence'.format(index+1))\n",
    "    print('\\tEN: {}'.format(en_sent))\n",
    "    print('\\tFR: {}'.format(fr_sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here are brief overview what steps will be done in this section\n",
    "\n",
    "- **create lookup tables** \n",
    "  - create two mapping tables \n",
    "      - (key, value) == (unique word string, its unique index)     - `(1)`\n",
    "      - (key, value) == (its unique index, unique word string)     - `(2)`\n",
    "      - `(1)` is used in the next step, and (2) is used later for prediction step\n",
    "      \n",
    "      \n",
    "- **text to word ids**\n",
    "  - convert each string word in the list of sentences to the index\n",
    "  - `(1)` is used for converting process\n",
    "  \n",
    "  \n",
    "- **save the pre-processed data**\n",
    "  - create two `(1)` mapping tables for English and French\n",
    "  - using the mapping tables, replace strings in the original source and target dataset with indicies\n",
    "\n",
    "### Create Lookup Tables\n",
    "\n",
    "As mentioned breifly, I am going to implement a function to create lookup tables. Since every models are mathmatically represented, the input and the output(prediction) should also be represented as numbers. That is why this step is necessary for NLP problem because human readable text is not machine readable. This function takes a list of sentences and returns two mapping tables (dictionary data type). Along with the list of sentences, there are special tokens, `<PAD>`, `<EOS>`, `<UNK>`, and `<GO>` to be added in the mapping tables. \n",
    "\n",
    "- (key, value) == (unique word string, its unique index)     - `(1)`\n",
    "- (key, value) == (its unique index, unique word string)     - `(2)`\n",
    "\n",
    "`(1)` will be used in the next step, `test to word ids`, to find a match between word and its index. `(2)` is not used in pre-processing step, but `(2)` will be used later. After making a prediction, the sequences of words in the output sentence will be represented as their indicies. The predicted output is machine readable but not human readable. That is why we need `(2)` to convert each indicies of words back into human readable words in string.\n",
    "\n",
    "<br/>\n",
    "<img src='./lookup.png' alt='Drawing' width='70%'>\n",
    "\n",
    "#### References\n",
    "- [Why special tokens?](https://datascience.stackexchange.com/questions/26947/why-do-we-need-to-add-start-s-end-s-symbols-when-using-recurrent-neural-n)\n",
    "- [Python `enumerate`](https://docs.python.org/3/library/functions.html#enumerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Word Ids\n",
    "\n",
    "Two `(1)` lookup tables will be provided in `text_to_ids` functions as arguments. They will be used in the converting process for English(source) and French(target) respectively. This part is more like a programming part, so there are not much to mention. I will just go over few minor things to remember before jumping in.\n",
    "\n",
    "- original(raw) source & target datas contain a list of sentences\n",
    "  - they are represented as a string \n",
    "\n",
    "- the number of sentences are the same for English and French\n",
    " \n",
    "- by accessing each sentences, need to convert word into the corresponding index.\n",
    "  - each word should be stored in a list\n",
    "  - this makes the resuling list as a 2-D array ( row: sentence, column: word index )\n",
    "  \n",
    "- for every target sentences, special token, `<EOS>` should be inserted at the end\n",
    "  - this token suggests when to stop creating a sequence\n",
    "  \n",
    "<br/>\n",
    "<img src='./conversion.png' alt='Drawing' width='70%'>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = [source_vocab_to_int['<PAD>']] * max_source_sentence_length\n",
    "        target_token_id = [target_vocab_to_int['<PAD>']] * max_target_sentence_length\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id[index] =  source_vocab_to_int[token]\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id[index] = target_vocab_to_int[token]\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Save Data\n",
    "\n",
    "`create_lookup_tables`, `text_to_ids` are generalized functions. It can really be used for other languages too. In this particular project, the target languages are English and French, so those languages have to fed into `create_lookup_tables`, `text_to_ids` functions to generate pre-processed dataset for this project. Here is the step to do it.\n",
    "\n",
    "- Load data(text) from the original file for English and French\n",
    "- Make them lower case letters\n",
    "- Create lookup tables for both English and French\n",
    "- Convert the original data into the list of sentences whose words are represented in index\n",
    "- Finally, save the preprocessed data to the external file (checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (English, French)\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    # to the lower case\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for English and French data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "\n",
    " This project uses a small set of sentences. However, in general, NLP requires a huge amount of raw text data. It would take quite a long time to preprocess, so it is recommended to avoid whenever possible. In practice, save the preprocessed data to the external file could speed up your job and let you focus more on building a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Environment (TensorFlow and GPU)\n",
    "\n",
    "Since the Recurrent Neural Networks is kind of heavy model to train, it is recommended to train the model in GPU environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Miniconda2\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    " In this notebook, I am going to build a special kind of model called 'sequence to sequence' (seq2seq in short). You can separate the entire model into 2 small sub-models. The first sub-model is called as __[E]__ Encoder, and the second sub-model is called as __[D]__ Decoder. __[E]__ takes a raw input text data just like any other RNN architectures do. At the end, __[E]__ outputs a neural representation. This is a very typical work, but you need to pay attention what this output really is. The output of __[E]__ is going to be the input data for __[D]__.\n",
    "\n",
    "That is why we call __[E]__ as Encoder and __[D]__ as Decoder. __[E]__ makes an output encoded in neural representational form, and we don't know what it really is. It is somewhat encrypted. __[D]__ has the ability to look inside the __[E]__'s output, and it will create a totally different output data (translated in French in this case). \n",
    "\n",
    "In order to build such a model, there are 4 steps to do overall.\n",
    "- __(1)__ make Input to Encoder networks\n",
    "- __(2)__ build Encoder networks and retrieve the output\n",
    "- __(3)__ make Input to Decoder networks (may need to add special characters. explained in more detail later)\n",
    "- __(4)__ build Decoder networks and retrieve the output\n",
    "\n",
    "Those step is conceptually right, be we should even break each steps into finer granularity for actual implementation. Only additional steps are highlighted in bold text style. Along with each detailed steps, I will list function names that I am going to implement.\n",
    "\n",
    "\n",
    "\n",
    "- `model_inputs`\n",
    "- `process_decoding_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "(2018/04/18)\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "\n",
    "Return the placeholders in the following the tuple (Input, Targets, Learing Rate, Keep Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(source_seq_length, target_seq_length):\n",
    "    # 1st None: batch\n",
    "    # 2nd None: the sequence of inputs (the length varies from sentence to sentence)\n",
    "    input = tf.placeholder(tf.int32, [None, source_seq_length], name=\"input\")\n",
    "    \n",
    "    # 1st None: batch\n",
    "    # 2nd None: same reason (the length of matching translated sentences varies in size - not same as the input sentence either)\n",
    "    targets = tf.placeholder(tf.int32, [None, target_seq_length], name=\"targets\")\n",
    "    \n",
    "    # simply floating value for learning_rate (set this value, so that it could be changed over time)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    return input, targets, learning_rate, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoding Input\n",
    "\n",
    "<img src=\"./training_phase.png\" style=\"width:400px;\"/>\n",
    "\n",
    "The figure above is borrowed from Thang Luong's thesis ['Neural Machine Translation'](https://github.com/lmthang/thesis/blob/master/thesis.pdf)\n",
    "\n",
    "Alternately, [tf.contrib.seq2seq.TrainingHelper](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    # TF strided_slice (https://www.tensorflow.org/api_docs/python/tf/strided_slice)\n",
    "    # -- extracts a strided slice of a tensor (generalized python array indexing).\n",
    "    #   -- can be thought as splitting into multiple tensors with the striding window size from begin to end\n",
    "    # -- arguments: TF Tensor, Begin, End, Strides\n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    \n",
    "    # TF concat (https://www.tensorflow.org/api_docs/python/tf/concat)\n",
    "    # -- Concatenates tensors along one dimension.\n",
    "    # -- arguments: a list of TF Tensor (tf.fill and after_slice in this case), axis=1\n",
    "    \n",
    "    # TF fill (https://www.tensorflow.org/api_docs/python/tf/fill)\n",
    "    # -- Creates a tensor filled with a scalar value.\n",
    "    # -- arguments: TF Tensor (must be int32/int64), value to fill\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer using [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: RNN state\n",
    "    \"\"\"\n",
    "    # TensorFlow Architecture (https://www.tensorflow.org/extend/architecture#overview)\n",
    "    # TensorFlow Fused Ops (https://www.tensorflow.org/performance/performance_guide#common_fused_ops)\n",
    "    # what is kernel in TensorFlow? (https://www.tensorflow.org/extend/adding_an_op#implement-the-kernel-for-the-op)\n",
    "    # RNN cell category (https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cell_wrappers_RNNCells_that_wrap_other_RNNCells_)\n",
    "    \n",
    "    # create one LSTM cell\n",
    "    # -- LSTM in blog (http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "    # -- BasicLSTMCell (https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "    #     - It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline. (threshold)\n",
    "    # -- LSTMCell (https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n",
    "    # -- LSTMBlockCell (https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockCell)\n",
    "    # -- LSTMBlockFusedCell (https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell)\n",
    "    # -- CudnnLSTM (https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM)\n",
    "    # \n",
    "    # TensorFlow RNN Performance (official doc)\n",
    "    # -- https://www.tensorflow.org/performance/performance_guide#rnn_performance\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    # make an array of LSTM cells in length of num_layers\n",
    "    # -- https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "#     print('enc_cell: {}'.format(enc_cell))\n",
    "    \n",
    "    # add dropout\n",
    "    # -- https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper\n",
    "    enc_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, keep_prob)\n",
    "    \n",
    "    # dynamic_rnn (https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "    # -- Performs fully dynamic unrolling of inputs.\n",
    "    print('rnn_inputs: {}'.format(rnn_inputs))\n",
    "\n",
    "    _, enc_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create training logits using [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder).  Apply the `output_fn` to the [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Train Logits\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, keep_prob)\n",
    "    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input,\n",
    "                                                              sequence_length,\n",
    "                                                              scope=decoding_scope)\n",
    "    logits = output_fn(train_pred)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference logits using [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param maximum_length: The maximum allowed time steps to decode\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param decoding_scope: TensorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Inference Logits\n",
    "    \"\"\"\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn, \n",
    "                                                                     encoder_state,\n",
    "                                                                     dec_embeddings,\n",
    "                                                                     start_of_sequence_id, end_of_sequence_id,\n",
    "                                                                     maximum_length, vocab_size)\n",
    "    logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell,\n",
    "                                                         infer_decoder_fn,\n",
    "                                                         scope=decoding_scope)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "- Create RNN cell for decoding using `rnn_size` and `num_layers`.\n",
    "- Create the output fuction using [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to transform it's input, logits, to class logits.\n",
    "- Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` function to get the training logits.\n",
    "- Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Miniconda2\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "# from tensorflow.layers import Dense\n",
    "\n",
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param encoder_state: The encoded state\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "#     dec_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    \n",
    "    train_helper = TrainingHelper(dec_embed_input, sequence_length)\n",
    "\n",
    "    projection_layer = tf.layers.Dense(len(target_vocab_to_int))\n",
    "\n",
    "    train_decoder = BasicDecoder(cell, \n",
    "                                 train_helper, \n",
    "                                 encoder_state,\n",
    "                                output_layer=projection_layer)\n",
    "\n",
    "    dec_train_logits, _, _ = dynamic_decode(train_decoder)\n",
    "    \n",
    "    # inference\n",
    "    infer_helper = GreedyEmbeddingHelper(dec_embeddings, tf.fill([batch_size], target_vocab_to_int['<GO>']), target_vocab_to_int['<EOS>'])\n",
    "\n",
    "    infer_decoder = BasicDecoder(cell, \n",
    "                                 infer_helper, \n",
    "                                 encoder_state,\n",
    "                                output_layer=projection_layer)\n",
    "    \n",
    "    dec_infer_logits, _, _ = dynamic_decode(infer_decoder)\n",
    "    \n",
    "#     with tf.variable_scope('decoding') as decoding_scope:\n",
    "#         output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "#                                                                 vocab_size, \n",
    "#                                                                 activation_fn=None,\n",
    "#                                                                 scope=decoding_scope)\n",
    "    \n",
    "#     with tf.variable_scope('decoding') as decoding_scope:\n",
    "#         dec_train_logits = decoding_layer_train(encoder_state,\n",
    "#                                                 dec_cell,\n",
    "#                                                 dec_embed_input, \n",
    "#                                                 sequence_length, \n",
    "#                                                 decoding_scope, \n",
    "#                                                 output_fn, \n",
    "#                                                 keep_prob)\n",
    "\n",
    "#     decoding_scope.reuse_variables() # with tf.variable_scope('decoding', reuse=True) as decoding_scope:\n",
    "#     dec_infer_logits = decoding_layer_infer(encoder_state, \n",
    "#                                             dec_cell, \n",
    "#                                             dec_embeddings, \n",
    "#                                             target_vocab_to_int['<GO>'],\n",
    "#                                             target_vocab_to_int['<EOS>'], \n",
    "#                                             sequence_length-1, # <EOS> not needed for inference\n",
    "#                                             vocab_size, \n",
    "#                                             decoding_scope, \n",
    "#                                             output_fn, \n",
    "#                                             keep_prob)\n",
    "    \n",
    "    return dec_train_logits, dec_infer_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Apply embedding to the input data for the encoder.\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob)`.\n",
    "- Process target data using your `process_decoding_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Apply embedding to the target data for the decoder.\n",
    "- Decode the encoded input using your `decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, enc_embedding_size)\n",
    "    print(enc_embed_input)\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob)\n",
    "    \n",
    "    dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state,\n",
    "                                                target_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                target_vocab_to_int, \n",
    "                                                keep_prob,\n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 256\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 200 \n",
    "decoding_embedding_size = 200 \n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 17, 200), dtype=float32)\n",
      "rnn_inputs: Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 17, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = pickle.load(open('preprocess.p', mode='rb'))\n",
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "source_seq_length = len(source_int_text[0])\n",
    "target_seq_length = len(target_int_text[0])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs(source_seq_length, target_seq_length)\n",
    "    target_length = tf.placeholder(tf.int32, (None,), name='target_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]), \n",
    "                                                   targets, \n",
    "                                                   keep_prob, \n",
    "                                                   batch_size, \n",
    "                                                   target_length, \n",
    "                                                   len(source_vocab_to_int), \n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size, \n",
    "                                                   decoding_embedding_size, \n",
    "                                                   rnn_size, num_layers, \n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "#     tf.identity(inference_logits, 'logits')\n",
    "#     with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "#     crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=train_logits.rnn_output)\n",
    "#     train_loss = (tf.reduce_sum(crossent * tf.ones([input_shape[0], target_seq_length])) / batch_size)\n",
    "    \n",
    "    \n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits.rnn_output,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], target_seq_length]))\n",
    "\n",
    "        # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"\n",
    "    Pad sentence with <PAD> id\n",
    "    \"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['<PAD>']] * (max_sentence - len(sentence))\n",
    "            for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(source, target, batch_size):\n",
    "    \"\"\"\n",
    "    Batch source and target together\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield (np.asarray(source_batch), np.asarray(target_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "#             _, loss = sess.run(\n",
    "#                 [train_op, cost],\n",
    "#                 {input_data: source_batch,\n",
    "#                  targets: target_batch,\n",
    "#                  lr: learning_rate,\n",
    "#                  target_length: [target_seq_length] * batch_size,\n",
    "#                  keep_prob: keep_probability})\n",
    "            \n",
    "            batch_train_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: source_batch, keep_prob: 1.0})\n",
    "            \n",
    "#             batch_valid_logits = sess.run(\n",
    "#                 inference_logits,\n",
    "#                 {input_data: valid_source, keep_prob: 1.0})\n",
    "                \n",
    "#             train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "#             valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "#             end_time = time.time()\n",
    "#             print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "#                   .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "pickle.dump(save_path, open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = pickle.load(open('preprocess.p', mode='rb'))\n",
    "load_path = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence\n",
    "To feed a sentence into the model for translation, you first need to preprocess it.  Implement the function `sentence_to_seq()` to preprocess new sentences.\n",
    "\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    " - Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for token in sentence.split(' '):\n",
    "        if (token in vocab_to_int):\n",
    "            output.append(vocab_to_int[token])\n",
    "        else:\n",
    "            output.append(vocab_to_int['<UNK>'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [136, 157, 103, 34, 63, 156, 188]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [129, 220, 346, 305, 338, 102, 165, 1]\n",
      "  French Words: ['il', 'a', 'vu', 'au', 'automobile', 'jaune', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperfect Translation\n",
    "You might notice that some sentences translate better than others.  Since the dataset you're using only has a vocabulary of 227 English words of the thousands that you use, you're only going to see good results using these words.  For this project, you don't need a perfect translation. However, if you want to create a better translation model, you'll need better data.\n",
    "\n",
    "You can train on the [WMT10 French-English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar).  This dataset has more vocabulary and richer in topics discussed.  However, this will take you days to train, so make sure you've a GPU and the neural network is performing well on dataset we provided.  Just make sure you play with the WMT10 corpus after you've submitted this project.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_language_translation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
